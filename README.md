# ğŸ” SEARCH_JOB â€“ Sistema Inteligente de ExtracciÃ³n y ClasificaciÃ³n de Empleo

`SEARCH_JOB` es una plataforma de anÃ¡lisis inteligente diseÃ±ada para **extraer, procesar, analizar y clasificar ofertas de empleo** desde mÃºltiples fuentes web, con el objetivo de generar conocimiento estructurado y detectar patrones relevantes del mercado laboral. Este sistema combina **scraping, procesamiento lingÃ¼Ã­stico, modelado machine learning y visualizaciÃ³n de datos**, todo orquestado en una arquitectura modular y escalable.

---

## ğŸ“Œ Objetivos principales

- Automatizar la extracciÃ³n de ofertas de empleo de mÃºltiples portales web.
- Limpiar, transformar y estandarizar la informaciÃ³n.
- Detectar y etiquetar entidades clave (tÃ­tulo, salario, empresa, ubicaciÃ³n, tecnologÃ­as, etc.).
- Clasificar las ofertas segÃºn distintos modelos (ej. NLP, catboost).
- Visualizar insights relevantes y exportar reportes.
- Guardar la informaciÃ³n estructurada en una base de datos relacional PostgreSQL.

---

## âš™ï¸ TecnologÃ­as y herramientas utilizadas

| CategorÃ­a | Herramientas |
|----------|--------------|
| Lenguaje principal | `Python 3.12` |
| Scraping | `Scrapy`, `requests`, `BeautifulSoup` |
| Procesamiento de texto | `spaCy`, `NLTK`, `re` |
| Modelado ML | `CatBoost`, `Scikit-learn`, `Joblib` |
| VisualizaciÃ³n | `Matplotlib`, `Seaborn`, `Plotly`, `PDF` |
| Base de datos | `PostgreSQL`, `psycopg2` |
| Entorno virtual | `venv_jobs` |
| Control de versiones | `Git`, `.gitignore` |
| OrganizaciÃ³n | `scripts/`, `models/`, `data/`, `reports/`, `config/`, `docs/` |
| Infraestructura | `.env` para credenciales, carpetas excluidas con `.gitignore` |

---

## ğŸ§± Estructura del proyecto

```
SEARCH_JOB/
â”œâ”€â”€ .github/                      # Workflows o acciones automatizadas
â”œâ”€â”€ .gitignore                   # Exclusiones de Git
â”œâ”€â”€ artifacts/                   # Resultados intermedios o modelos
â”œâ”€â”€ config/                      # ConfiguraciÃ³n y scripts de limpieza
â”‚   â””â”€â”€ reorganizar_estructura_jobs.ps1
â”œâ”€â”€ data/                        # Conjuntos de datos estructurados
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”œâ”€â”€ docs/                        # DocumentaciÃ³n tÃ©cnica o referencias
â”œâ”€â”€ img/                         # GrÃ¡ficos usados en informes o presentaciones
â”œâ”€â”€ models/                      # Modelos entrenados y recursos NLP
â”‚   â””â”€â”€ ner_model/, ner_ng/, catboost_info/
â”œâ”€â”€ reports/                     # Informes generados (PDFs, visualizaciones)
â”œâ”€â”€ scripts/                     # CÃ³digo fuente modular
â”‚   â”œâ”€â”€ scraping/
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ nlp/
â”‚   â”œâ”€â”€ feature_engineering/
â”‚   â”œâ”€â”€ model_training/
â”‚   â””â”€â”€ analysis/
â”œâ”€â”€ scrapy_employment_scraper/  # Proyecto Scrapy para fuentes web
â”œâ”€â”€ .env                         # Variables de entorno (NO subir a GitHub)
â””â”€â”€ README.md                    # Este archivo
```

---

## ğŸ§  Modelos incluidos

- **NER personalizado (spaCy)**: Reconocimiento de entidades como "Empresa", "TecnologÃ­a", "UbicaciÃ³n", "Salario".
- **CatBoostClassifier**: Modelo entrenado para clasificar ofertas de empleo por sector o tipo de perfil.
- **Reglas heurÃ­sticas y limpieza avanzada**: Para correcciÃ³n de ruido textual y deduplicaciÃ³n.

---

## ğŸš€ EjecuciÃ³n del sistema

1. Clona el repositorio y activa el entorno virtual `venv_jobs`.
2. Crea un archivo `.env` con tus credenciales PostgreSQL:

```env
DB_HOST=
DB_NAME=
DB_USER=
DB_PASS=
```

3. Ejecuta los mÃ³dulos de scraping desde `scrapy_employment_scraper/`.
4. Lanza el pipeline de limpieza, modelado y anÃ¡lisis desde `scripts/`.

---

## ğŸ“¤ ExportaciÃ³n y visualizaciÃ³n

- Los resultados se exportan a PDF (`reports/`) y se pueden consultar en dashboards personalizados.
- Los modelos y datos se almacenan localmente en `models/` y `data/`.

---

## âŒ Exclusiones importantes (.gitignore)

```bash
# Entornos virtuales
venv_jobs/
.env

# Modelos pesados
models/**/tokenizer.json
models/**/vocab.txt
catboost_info/
*.joblib

# Archivos intermedios o binarios
*.pdf
*.pt
*.tsv
*.tmp
__pycache__/
```

---

âœ… Estado del Proyecto: Plataforma de RecomendaciÃ³n Formativa Basada en Empleo Real
ğŸŸ¢ Fase 1. AnÃ¡lisis, planificaciÃ³n y diseÃ±o
Tarea	Estado
AnÃ¡lisis del objetivo (identificar formaciones con mayor impacto salarial)	âœ… Completado
InvestigaciÃ³n de fuentes de datos (plataformas de empleo globales)	âœ… Completado
SelecciÃ³n de arquitectura general del sistema	âœ… Completado
DefiniciÃ³n de flujos de trabajo y mÃ³dulos funcionales	âœ… Completado
Herramientas seleccionadas (Scrapy, Playwright, ML, SHAP, Streamlitâ€¦)	âœ… Completado

ğŸŸ¢ Fase 2. IngenierÃ­a de scraping y recolecciÃ³n de datos
Tarea	Estado
IdentificaciÃ³n de las 10 principales plataformas de empleo global	âœ… Completado
DiseÃ±o de scraper modular con tolerancia a fallos	âœ… Completado
Scraping funcional con Playwright (dinÃ¡mico, multi-plataforma)	ğŸŸ¡ En desarrollo avanzado
NormalizaciÃ³n de datos extraÃ­dos (HTML â†’ JSON/CSV limpios)	ğŸŸ¡ En desarrollo avanzado
GestiÃ³n de bloqueos (proxies, headers, retries, captchas)	ğŸŸ¡ En desarrollo
Guardado en estructura local CSV o MongoDB	ğŸ”µ Listo para implementar

ğŸŸ¢ Fase 3. Procesamiento de lenguaje natural (NLP)
Tarea	Estado
Limpieza de descripciones (remociÃ³n HTML, stopwords, etc.)	ğŸ”µ Listo para implementar
DetecciÃ³n de entidades clave (skills, certs, grados)	ğŸ”µ Listo para implementar
Fine-tuning de modelo BERT o uso de spaCy/NER preentrenado	ğŸ”µ Listo para implementar
NormalizaciÃ³n semÃ¡ntica (sinÃ³nimos, agrupaciones)	ğŸ”µ Listo para implementar
GeneraciÃ³n de dataset estructurado	ğŸ”µ Listo para implementar

ğŸŸ¡ Fase 4. Modelado predictivo de salarios
Tarea	Estado
IngenierÃ­a de features (dummies, embeddings, experiencia, etc.)	ğŸ”µ Planificado
Entrenamiento con XGBoost, LightGBM, redes neuronales	ğŸ”µ Planificado
OptimizaciÃ³n con Optuna / CV	ğŸ”µ Planificado
ValidaciÃ³n con MAE, RÂ²	ğŸ”µ Planificado
Explicabilidad con SHAP	ğŸ”µ Planificado
Ensemble final con modelos combinados	ğŸ”µ Planificado

ğŸŸ¢ Fase 5. RecomendaciÃ³n personalizada y ROI
Tarea	Estado
SimulaciÃ³n de escenarios (aÃ±adir formaciones y estimar incremento)	ğŸ”µ Listo para implementar
CÃ¡lculo de ROI y eficiencia de cada formaciÃ³n	ğŸ”µ Listo para implementar
GeneraciÃ³n automÃ¡tica de ranking personalizado	ğŸ”µ Listo para implementar

ğŸŸ¢ Fase 6. VisualizaciÃ³n, reporting y API
Tarea	Estado
DiseÃ±o del dashboard en Streamlit	ğŸ”µ Listo para implementar
VisualizaciÃ³n de resultados y simulaciones	ğŸ”µ Listo para implementar
GeneraciÃ³n de informes PDF ejecutivos	ğŸ”µ Listo para implementar
API REST con FastAPI (predicciÃ³n y sugerencia)	ğŸ”µ Listo para implementar

ğŸŸ¢ Fase 7. Despliegue, automatizaciÃ³n y monitoreo
Tarea	Estado
ContenerizaciÃ³n con Docker	ğŸ”µ Planificado
PlanificaciÃ³n diaria con Airflow / Task Scheduler	ğŸ”µ Planificado
Sistema de logs y errores	ğŸ”µ Planificado
MonitorizaciÃ³n y retraining automÃ¡tico	ğŸ”µ Planificado

ğŸ“Œ Resumen por estado
Estado	Tareas clave (resumen)
âœ… Completado	DiseÃ±o general, selecciÃ³n tecnolÃ³gica, definiciÃ³n de arquitectura, anÃ¡lisis semÃ¡ntico
ğŸŸ¡ En desarrollo	Scraper multi-plataforma (Playwright), normalizaciÃ³n de scraping
ğŸ”µ Listo para implementar	NLP (NER), modelado, recomendaciÃ³n, dashboard, API, reporting, despliegue
ğŸ”´ Pendiente	Retraining automÃ¡tico, integraciÃ³n total de mÃ³dulos

ğŸš€ Siguiente paso recomendado
Finalizar la fase de scraping robusto para las 10 plataformas, almacenar los datos en CSV o MongoDB, y comenzar inmediatamente con el mÃ³dulo NLP (extracciÃ³n de entidades) para preparar el dataset final de entrenamiento.




---

