
---

````markdown
# ğŸ” JOB SEARCH â€“ Sistema Inteligente de ExtracciÃ³n, AnÃ¡lisis y RecomendaciÃ³n de Empleo

**JOB SEARCH** es una plataforma modular y escalable de anÃ¡lisis de empleo basada en datos reales. EstÃ¡ diseÃ±ada para extraer automÃ¡ticamente ofertas laborales desde mÃºltiples portales, analizar descripciones con tÃ©cnicas de NLP, predecir salarios utilizando modelos de machine learning, y recomendar formaciones con mayor retorno de inversiÃ³n. Su arquitectura se basa en scraping avanzado, procesamiento semÃ¡ntico, aprendizaje automÃ¡tico e interfaces interactivas.

---

## ğŸ¯ Objetivos del sistema

- Extraer automÃ¡ticamente ofertas desde portales globales como LinkedIn o Indeed.
- Limpiar, transformar y estructurar los datos textuales con precisiÃ³n semÃ¡ntica.
- Detectar entidades clave (puesto, empresa, tecnologÃ­a, salario...).
- Clasificar y analizar ofertas con modelos ML supervisados y reglas heurÃ­sticas.
- Visualizar informaciÃ³n clave mediante dashboards y generar informes PDF.
- Estimar el ROI de formaciones segÃºn impacto en salario esperado.
- Recomendar cursos con mayor beneficio potencial.
- *(Opcional)* Almacenar los datos en MongoDB o PostgreSQL.

---

## âš™ï¸ TecnologÃ­as utilizadas

| Ãrea                   | Herramientas principales                                                 |
|------------------------|---------------------------------------------------------------------------|
| Lenguaje principal     | Python 3.12                                                               |
| Web Scraping           | Scrapy, Playwright, BeautifulSoup, requests, proxies dinÃ¡micos           |
| Procesamiento NLP      | spaCy, transformers, re, nltk, sklearn, pandas                           |
| ExtracciÃ³n de entidades| NER personalizado, BERT, spaCy                                           |
| Modelado Predictivo    | XGBoost, LightGBM, Optuna, SHAP, CatBoost                                |
| RecomendaciÃ³n & ROI    | Algoritmo personalizado de ranking y simulaciÃ³n de impacto formativo     |
| VisualizaciÃ³n          | Streamlit, Plotly, Matplotlib, Seaborn                                  |
| Backend & API          | FastAPI, Pydantic, Uvicorn                                               |
| AutomatizaciÃ³n         | Airflow, Task Scheduler, Docker *(planificado)*                         |
| Almacenamiento         | CSV, JSON, MongoDB, PostgreSQL *(opcional)*                             |
| Entornos y versiones   | .env, venv, .gitignore, Git                                              |

---

## ğŸ§± Estructura del Proyecto

```bash
SEARCH_JOB/
â”œâ”€â”€ config/                      # ConfiguraciÃ³n, YAMLs, scripts de entorno
â”œâ”€â”€ data/                        # Datos estructurados, CSVs, procesados (no versionados)
â”œâ”€â”€ docs/                        # DocumentaciÃ³n tÃ©cnica y funcional
â”œâ”€â”€ img/                         # Visualizaciones y recursos grÃ¡ficos
â”œâ”€â”€ models/                      # Modelos entrenados (NER, clasificadores)
â”œâ”€â”€ reports/                     # Informes en PDF generados automÃ¡ticamente
â”œâ”€â”€ scripts/                     # CÃ³digo fuente organizado por funciÃ³n
â”‚   â”œâ”€â”€ scraping/                # Scrapers (Scrapy, Playwright)
â”‚   â”œâ”€â”€ preprocessing/           # Limpieza de texto, normalizaciÃ³n
â”‚   â”œâ”€â”€ nlp/                     # ExtracciÃ³n de entidades (NER)
â”‚   â”œâ”€â”€ model_training/          # Entrenamiento y validaciÃ³n de modelos ML
â”‚   â”œâ”€â”€ prediction/              # PredicciÃ³n y clasificaciÃ³n de nuevas ofertas
â”‚   â”œâ”€â”€ feature_engineering/     # GeneraciÃ³n y anÃ¡lisis de variables
â”‚   â”œâ”€â”€ dashboard/               # VisualizaciÃ³n en Streamlit
â”‚   â””â”€â”€ informe/                 # Informes PDF y visualizaciones ejecutivas
â”œâ”€â”€ scrapy_employment_scraper/  # Proyecto Scrapy funcional
â”œâ”€â”€ .env                         # Variables de entorno privadas
â”œâ”€â”€ requirements.txt             # Dependencias del sistema
â””â”€â”€ README.md                    # DescripciÃ³n general del sistema
````

---

## ğŸ§  Modelos y lÃ³gica implementada

### ğŸ”¹ NER (Reconocimiento de Entidades Nombradas)

* ExtracciÃ³n de: empresa, tecnologÃ­a, ubicaciÃ³n, nivel, experiencia, idiomas, salario.
* Modelos basados en `spaCy` y `transformers` (BERT).

### ğŸ”¹ ClasificaciÃ³n y predicciÃ³n

* Clasificadores (ej. `CatBoost`) para tipos de empleo.
* Modelos regresivos (`XGBoost`, `LightGBM`) para predicciÃ³n salarial.
* OptimizaciÃ³n con `Optuna`.
* Explicabilidad mediante `SHAP`.

### ğŸ”¹ Recomendador con ROI

* Algoritmo que estima el incremento salarial tras realizar una formaciÃ³n.
* SimulaciÃ³n de escenarios y ranking de cursos con mayor retorno.

---

## ğŸš€ EjecuciÃ³n del sistema

```bash
# 1. Clona el repositorio y crea el entorno
git clone https://github.com/Alexmurfitt/JOB-SEARCH.git
cd JOB-SEARCH
python -m venv venv_jobs
.\venv_jobs\Scripts\activate

# 2. Instala las dependencias reales del sistema
pip install -r requirements.txt

# 3. Configura tu archivo .env si vas a usar MongoDB o PostgreSQL
```

---

## ğŸ”§ EjecuciÃ³n modular

* **Scraping** â†’ `scripts/scraping/` o `scrapy_employment_scraper/`
* **Preprocesado** â†’ `scripts/preprocessing/`
* **NER y NLP** â†’ `scripts/nlp/`
* **Modelado** â†’ `scripts/model_training/`
* **RecomendaciÃ³n y ROI** â†’ `scripts/prediction/`
* **Dashboard** â†’ `scripts/dashboard/app.py`
* **Informes PDF** â†’ `scripts/informe/generar_informe_data_analyst.py`

---

## ğŸ“¤ ExportaciÃ³n y VisualizaciÃ³n

* Resultados exportados a `reports/` (PDFs, grÃ¡ficos).
* Interfaz opcional en Streamlit para exploraciÃ³n interactiva.
* Datos intermedios y finales almacenados en `data/` y `models/`.

---

## ğŸ›¡ï¸ Exclusiones clave (.gitignore)

```gitignore
# Entornos y secretos
.venv/
.env

# Archivos pesados o no versionables
*.joblib
*.pkl
*.pt
*.bin
*.pdf
*.tsv
*.tmp
*.log
__pycache__/
models/**/tokenizer.json
models/**/vocab.txt
catboost_info/
data/
artifacts/
```
---

# âœ… Estado del Proyecto: Plataforma de RecomendaciÃ³n Formativa Basada en Empleo Real

---

## ğŸŸ¢ Fase 1. AnÃ¡lisis, planificaciÃ³n y diseÃ±o

| Tarea                                                                 | Estado         |
|-----------------------------------------------------------------------|----------------|
| AnÃ¡lisis del objetivo (identificar formaciones con mayor impacto salarial) | âœ… Completado |
| InvestigaciÃ³n de fuentes de datos (plataformas de empleo globales)   | âœ… Completado   |
| SelecciÃ³n de arquitectura general del sistema                        | âœ… Completado   |
| DefiniciÃ³n de flujos de trabajo y mÃ³dulos funcionales                | âœ… Completado   |
| Herramientas seleccionadas (Scrapy, Playwright, ML, SHAP, Streamlitâ€¦) | âœ… Completado  |

---

## ğŸŸ¢ Fase 2. IngenierÃ­a de scraping y recolecciÃ³n de datos

| Tarea                                                               | Estado                   |
|---------------------------------------------------------------------|--------------------------|
| IdentificaciÃ³n de las 10 principales plataformas de empleo global   | âœ… Completado            |
| DiseÃ±o de scraper modular con tolerancia a fallos                   | âœ… Completado            |
| Scraping funcional con Playwright (dinÃ¡mico, multi-plataforma)      | ğŸŸ¡ En desarrollo avanzado |
| NormalizaciÃ³n de datos extraÃ­dos (HTML â†’ JSON/CSV limpios)          | ğŸŸ¡ En desarrollo avanzado |
| GestiÃ³n de bloqueos (proxies, headers, retries, captchas)           | ğŸŸ¡ En desarrollo          |
| Guardado en estructura local CSV o MongoDB                          | ğŸ”µ Listo para implementar |

---

## ğŸŸ¢ Fase 3. Procesamiento de lenguaje natural (NLP)

| Tarea                                                           | Estado                   |
|-----------------------------------------------------------------|--------------------------|
| Limpieza de descripciones (remociÃ³n HTML, stopwords, etc.)      | ğŸ”µ Listo para implementar |
| DetecciÃ³n de entidades clave (skills, certs, grados)            | ğŸ”µ Listo para implementar |
| Fine-tuning de modelo BERT o uso de spaCy/NER preentrenado      | ğŸ”µ Listo para implementar |
| NormalizaciÃ³n semÃ¡ntica (sinÃ³nimos, agrupaciones)               | ğŸ”µ Listo para implementar |
| GeneraciÃ³n de dataset estructurado                              | ğŸ”µ Listo para implementar |

---

## ğŸŸ¡ Fase 4. Modelado predictivo de salarios

| Tarea                                                             | Estado        |
|-------------------------------------------------------------------|---------------|
| IngenierÃ­a de features (dummies, embeddings, experiencia, etc.)   | ğŸ”µ Planificado |
| Entrenamiento con XGBoost, LightGBM, redes neuronales             | ğŸ”µ Planificado |
| OptimizaciÃ³n con Optuna / CV                                      | ğŸ”µ Planificado |
| ValidaciÃ³n con MAE, RÂ²                                            | ğŸ”µ Planificado |
| Explicabilidad con SHAP                                           | ğŸ”µ Planificado |
| Ensemble final con modelos combinados                             | ğŸ”µ Planificado |

---

## ğŸŸ¢ Fase 5. RecomendaciÃ³n personalizada y ROI

| Tarea                                                              | Estado                   |
|--------------------------------------------------------------------|--------------------------|
| SimulaciÃ³n de escenarios (aÃ±adir formaciones y estimar incremento) | ğŸ”µ Listo para implementar |
| CÃ¡lculo de ROI y eficiencia de cada formaciÃ³n                      | ğŸ”µ Listo para implementar |
| GeneraciÃ³n automÃ¡tica de ranking personalizado                     | ğŸ”µ Listo para implementar |

---

## ğŸŸ¢ Fase 6. VisualizaciÃ³n, reporting y API

| Tarea                                            | Estado                   |
|--------------------------------------------------|--------------------------|
| DiseÃ±o del dashboard en Streamlit                | ğŸ”µ Listo para implementar |
| VisualizaciÃ³n de resultados y simulaciones       | ğŸ”µ Listo para implementar |
| GeneraciÃ³n de informes PDF ejecutivos            | ğŸ”µ Listo para implementar |
| API REST con FastAPI (predicciÃ³n y sugerencia)   | ğŸ”µ Listo para implementar |

---

## ğŸŸ¢ Fase 7. Despliegue, automatizaciÃ³n y monitoreo

| Tarea                                               | Estado        |
|-----------------------------------------------------|---------------|
| ContenerizaciÃ³n con Docker                          | ğŸ”µ Planificado |
| PlanificaciÃ³n diaria con Airflow / Task Scheduler   | ğŸ”µ Planificado |
| Sistema de logs y errores                           | ğŸ”µ Planificado |
| MonitorizaciÃ³n y retraining automÃ¡tico              | ğŸ”µ Planificado |

---

## ğŸ“Œ Resumen por estado

| Estado        | Tareas clave                                                                 |
|---------------|-------------------------------------------------------------------------------|
| âœ… Completado | DiseÃ±o general, selecciÃ³n tecnolÃ³gica, definiciÃ³n de arquitectura, anÃ¡lisis semÃ¡ntico |
| ğŸŸ¡ En desarrollo | Scraper multi-plataforma (Playwright), normalizaciÃ³n de scraping               |
| ğŸ”µ Listo para implementar | NLP (NER), modelado, recomendaciÃ³n, dashboard, API, reporting, despliegue     |
| ğŸ”´ Pendiente   | Retraining automÃ¡tico, integraciÃ³n total de mÃ³dulos                           |

---

## ğŸš€ Siguiente paso recomendado

Finalizar la fase de scraping robusto para las 10 plataformas, almacenar los datos en CSV o MongoDB, y comenzar inmediatamente con el mÃ³dulo NLP (extracciÃ³n de entidades) para preparar el dataset final de entrenamiento.




---

